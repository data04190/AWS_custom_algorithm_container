#!/usr/bin/env python

# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import json
import os
import pickle
import sys
import argparse
import traceback
import numpy as np
from scipy import linalg 


# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.

channel_name='training'
training_path = os.path.join(input_path, channel_name)

# The function to execute the training.

def train(args):

    # Parse arguments
    trainLen = args.trainLen
    testLen = args.testLen
    initLen = args.initLen
  

    print('Starting the training.')
    
    try:
    
        # Read in any hyperparameters that the user passed with the training job
        
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)

        # Take the set of files and read them all into a single pandas dataframe       
        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        if len(input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
                              
        
        print(input_files)
        data = np.loadtxt('/opt/ml/input/data/training/MackeyGlass_t17.csv')
                              
        #trainLen = 2000
        #testLen = 2000
        #initLen = 100
               
        # generate the ESN reservoir

        inSize = outSize = 1
        resSize = 1000
        a = 0.3    # leaking rate

        np.random.seed(42)
        
        Win = (np.random.rand(resSize,1+inSize) - 0.5) * 1
        W = np.random.rand(resSize,resSize) - 0.5 
        
        # normalizing and setting spectral radius (correct, slow):
        print('Computing spectral radius...')
        rhoW = max(abs(linalg.eig(W)[0]))
        print('done.')
        W *= 1.25 / rhoW
        
        # allocated memory for the design (collected states) matrix
        X = np.zeros((1+inSize+resSize,trainLen-initLen))
    
        # set the corresponding target matrix directly
        Yt = data[None,initLen+1:trainLen+1] 
        
        # run the reservoir with the data and collect X
        x = np.zeros((resSize,1))
        for t in range(trainLen):
            u = data[t]
            x = (1-a)*x + a*np.tanh( np.dot( Win, np.vstack((1,u)) ) + np.dot( W, x ) )
            if t >= initLen:
                X[:,t-initLen] = np.vstack((1,u,x))[:,0]
                
        # train the output by ridge regression
        reg = 1e-8  # regularization coefficient
        
        # direct equations from texts:
        #X_T = X.T
        #Wout = np.dot( np.dot(Yt,X_T), linalg.inv( np.dot(X,X_T) + \
        #    reg*np.eye(1+inSize+resSize) ) )
        # using scipy.linalg.solve:
        Wout = linalg.solve( np.dot(X,X.T) + reg*np.eye(1+inSize+resSize), 
            np.dot(X,Yt.T) ).T
        
        # run the trained ESN in a generative mode. no need to initialize here, 
        # because x is initialized with training data and we continue from there.
        Y = np.zeros((outSize,testLen))
        u = data[trainLen]
        for t in range(testLen):
            x = (1-a)*x + a*np.tanh( np.dot( Win, np.vstack((1,u)) ) + np.dot( W, x ) )
            y = np.dot( Wout, np.vstack((1,u,x)) )
            Y[:,t] = y
            # generative mode:
            u = y
            ## this would be a predictive mode:
            #u = data[trainLen+t+1]        
            
            
        # compute MSE for the first errorLen time steps
        errorLen = 500
        mse = sum( np.square( data[trainLen+1:trainLen+errorLen+1] - 
            Y[0,0:errorLen] ) ) / errorLen
        print('MSE = ' + str( mse ))
        

        print('\nTraining complete.')
        
        
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

def parse_args():

    parser = argparse.ArgumentParser()
    
    # Hyperparameter Setting
    parser.add_argument('--trainLen', type=int, default=2000)
    parser.add_argument('--testLen', type=int, default=2000)    
    parser.add_argument('--initLen', type=int, default=100)
        
    args = parser.parse_args()
    return args            

if __name__ == '__main__':
    args = parse_args()
    train(args) 

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)